{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12c4259d-0ddb-472e-85d0-6d89b8f70521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e41d1136-652f-4fe8-8c55-63726113d113",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fairlearn in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (0.12.0)\nRequirement already satisfied: numpy>=1.24.4 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from fairlearn) (1.24.4)\nRequirement already satisfied: pandas>=2.0.3 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from fairlearn) (2.2.3)\nRequirement already satisfied: scikit-learn>=1.2.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from fairlearn) (1.4.2)\nRequirement already satisfied: scipy>=1.9.3 in /databricks/python3/lib/python3.11/site-packages (from fairlearn) (1.11.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /databricks/python3/lib/python3.11/site-packages (from pandas>=2.0.3->fairlearn) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.11/site-packages (from pandas>=2.0.3->fairlearn) (2022.7)\nRequirement already satisfied: tzdata>=2022.7 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from pandas>=2.0.3->fairlearn) (2025.2)\nRequirement already satisfied: joblib>=1.2.0 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn>=1.2.1->fairlearn) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn>=1.2.1->fairlearn) (2.2.0)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.3->fairlearn) (1.16.0)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting BlackBoxAuditing\n  Using cached BlackBoxAuditing-0.1.54-py2.py3-none-any.whl\nRequirement already satisfied: networkx in /databricks/python3/lib/python3.11/site-packages (from BlackBoxAuditing) (3.1)\nRequirement already satisfied: matplotlib in /databricks/python3/lib/python3.11/site-packages (from BlackBoxAuditing) (3.7.2)\nRequirement already satisfied: pandas in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from BlackBoxAuditing) (2.2.3)\nRequirement already satisfied: numpy in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from BlackBoxAuditing) (1.24.4)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib->BlackBoxAuditing) (1.0.5)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.11/site-packages (from matplotlib->BlackBoxAuditing) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib->BlackBoxAuditing) (4.25.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib->BlackBoxAuditing) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib->BlackBoxAuditing) (23.2)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib->BlackBoxAuditing) (9.4.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib->BlackBoxAuditing) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.11/site-packages (from matplotlib->BlackBoxAuditing) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.11/site-packages (from pandas->BlackBoxAuditing) (2022.7)\nRequirement already satisfied: tzdata>=2022.7 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from pandas->BlackBoxAuditing) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->BlackBoxAuditing) (1.16.0)\nInstalling collected packages: BlackBoxAuditing\nSuccessfully installed BlackBoxAuditing-0.1.54\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting aif360\n  Obtaining dependency information for aif360 from https://files.pythonhosted.org/packages/61/7b/7e4fa9e7b6f62759663db3b5aaa12a6cc9ef866223e5978c25844bceb762/aif360-0.6.1-py3-none-any.whl.metadata\n  Using cached aif360-0.6.1-py3-none-any.whl.metadata (5.0 kB)\nRequirement already satisfied: numpy>=1.16 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from aif360) (1.24.4)\nRequirement already satisfied: scipy>=1.2.0 in /databricks/python3/lib/python3.11/site-packages (from aif360) (1.11.1)\nRequirement already satisfied: pandas>=0.24.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from aif360) (2.2.3)\nRequirement already satisfied: scikit-learn>=1.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from aif360) (1.4.2)\nRequirement already satisfied: matplotlib in /databricks/python3/lib/python3.11/site-packages (from aif360) (3.7.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /databricks/python3/lib/python3.11/site-packages (from pandas>=0.24.0->aif360) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.11/site-packages (from pandas>=0.24.0->aif360) (2022.7)\nRequirement already satisfied: tzdata>=2022.7 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages (from pandas>=0.24.0->aif360) (2025.2)\nRequirement already satisfied: joblib>=1.2.0 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn>=1.0->aif360) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn>=1.0->aif360) (2.2.0)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib->aif360) (1.0.5)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.11/site-packages (from matplotlib->aif360) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib->aif360) (4.25.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib->aif360) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib->aif360) (23.2)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib->aif360) (9.4.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib->aif360) (3.0.9)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=0.24.0->aif360) (1.16.0)\nUsing cached aif360-0.6.1-py3-none-any.whl (259 kB)\nInstalling collected packages: aif360\nSuccessfully installed aif360-0.6.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 14:54:37.446713: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nWARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\npip install 'aif360[inFairness]'\nWARNING:root:No module named 'ot': ot_distance will be unavailable. To install, run:\npip install 'aif360[OptimalTransport]'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import  accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report,  precision_recall_curve, auc \n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, plot_importance \n",
    "\n",
    "!pip install fairlearn\n",
    "from fairlearn.metrics import MetricFrame, demographic_parity_difference, equalized_odds_difference, false_positive_rate, true_positive_rate, selection_rate\n",
    "\n",
    "!pip install BlackBoxAuditing\n",
    "\n",
    "!pip install aif360 \n",
    "from aif360.datasets import StandardDataset, BinaryLabelDataset\n",
    "from aif360.algorithms.preprocessing import Reweighing, DisparateImpactRemover\n",
    "from aif360.algorithms.inprocessing import ExponentiatedGradientReduction, AdversarialDebiasing\n",
    "from aif360.algorithms.postprocessing import RejectOptionClassification, EqOddsPostprocessing\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.sklearn.metrics import disparate_impact_ratio\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33db32fc-0e0b-42cb-9105-ed093b24ce05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Data Pre-Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0a7d1280-52d6-4cbc-a9c6-cfb6c0216e05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "########################################################################################################################################################\n",
    "## - Create clean pandas dataframe: df_cleaned_final\n",
    "\n",
    "# Define the file path to read in the raw data\n",
    "file_path = \"/Volumes/prod_adw/dev_final_claims/cahps/Data_incl_sens.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop rows with missing 'PCM_DETRACTOR' values\n",
    "df_cleaned = df.dropna(subset=['PCM_DETRACTOR'])\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = [\n",
    "    'STARS_ID', 'TARGETREPORTMONTH', 'PCM_DETRACTOR_RAW', 'PCM_DETRACTOR_SCALED', \n",
    "    'FEATUREREPORTMONTH', 'STATE', 'COUNTY', 'BBT_6M', 'HMG_COA_REDUCTASE_INHIBTORS_2_RETAIL_FLG_9M', 'PRIOR_YEAR_CONTRACT_STAR_RATING_CTM_D',\n",
    "    'STARS_ID.1', 'TARGETREPORTMONTH.1', 'PCM_DETRACTOR.1', \n",
    "    'PCM_DETRACTOR_RAW.1', 'PCM_DETRACTOR_SCALED.1', 'HEDIS_MSR_ADH_COUNT.1', 'AGE.1', \n",
    "    'RACE_ETHNICITY.1', 'GENDER.1','LANG.1','EDUC.1','CONTRACT_TENURE.1', 'COUNTY.1',\n",
    "    'STATE.1', 'PLANTYPE.1', 'SURVEY_MODE.1','SMOKENOW.1', 'SDOH_CATG_SOCIAL_ECONIMICAL_24M_FLG.1',\n",
    "    'SDOH_TRANSPORTATION_FOR_MEDICAL_APPOINTMENT_24M_FLG.1'\n",
    "]\n",
    "\n",
    "df_cleaned_final = df_cleaned.drop(columns=columns_to_drop)\n",
    "\n",
    "# Replace missing values in 'PROVIDER_RISK_HIERARCHY' with 'M'\n",
    "df_cleaned_final['PROVIDER_RISK_HIERARCHY'] = df_cleaned_final['PROVIDER_RISK_HIERARCHY'].fillna('M')\n",
    "\n",
    "# Convert 'PCM_DETRACTOR' to category\n",
    "df_cleaned_final['PCM_DETRACTOR'] = df_cleaned_final['PCM_DETRACTOR'].astype('category')\n",
    "\n",
    "# Convert specified columns to category, others to numeric\n",
    "category_columns = [\n",
    "    'PCM_DETRACTOR', 'PROVIDER_RISK_HIERARCHY', 'AGE', 'LANG', 'EDUC', \n",
    "    'RACE_ETHNICITY', 'PLANTYPE', 'SMOKENOW', 'GENDER', 'SURVEY_MODE'\n",
    "]\n",
    "for column in df_cleaned_final.columns:\n",
    "    if column in category_columns:\n",
    "        df_cleaned_final[column] = df_cleaned_final[column].astype('category')\n",
    "    else:\n",
    "        df_cleaned_final[column] = pd.to_numeric(df_cleaned_final[column], errors='coerce')\n",
    "\n",
    "\n",
    "# Replace missing values in numeric columns with the mean\n",
    "numeric_columns = df_cleaned_final.select_dtypes(include=[np.number]).columns\n",
    "df_cleaned_final[numeric_columns] = df_cleaned_final[numeric_columns].apply(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# Replace missing values in categorical columns with the mode\n",
    "categorical_columns = df_cleaned_final.select_dtypes(include=['category']).columns\n",
    "df_cleaned_final[categorical_columns] = df_cleaned_final[categorical_columns].apply(lambda x: x.fillna(x.mode()[0]))\n",
    "\n",
    "# Check for class imbalance\n",
    "class_counts = df_cleaned_final['PCM_DETRACTOR'].value_counts()\n",
    "class_percentages = df_cleaned_final['PCM_DETRACTOR'].value_counts(normalize=True) * 100\n",
    "\n",
    "\n",
    "########################################################################################################################################################\n",
    "## - Normalize numeric features\n",
    "## - Split into X_train, X_test, y_train, y_test\n",
    "## - These will be in pandas format\n",
    "\n",
    "## Note: One hot encoding has not yet been applied to categorical features\n",
    "\n",
    "# Separate the target variable and explanatory variables\n",
    "X = df_cleaned_final.drop(columns=['PCM_DETRACTOR'])\n",
    "y = df_cleaned_final['PCM_DETRACTOR']\n",
    "\n",
    "# Define the numeric features and the categorical features\n",
    "categorical_features = [\n",
    "    'PROVIDER_RISK_HIERARCHY', 'AGE', 'LANG', 'EDUC', 'RACE_ETHNICITY', \n",
    "    'PLANTYPE', 'SMOKENOW', 'GENDER', 'SURVEY_MODE'\n",
    "]\n",
    "numeric_features = X.columns.drop(categorical_features)\n",
    "\n",
    "# Convert non-numeric values to NaN and then handle missing values\n",
    "X[numeric_features] = X[numeric_features].apply(pd.to_numeric, errors='coerce')\n",
    "X[numeric_features] = X[numeric_features].fillna(X[numeric_features].mean())\n",
    "\n",
    "# Normalize the numeric features\n",
    "scaler = StandardScaler()\n",
    "X[numeric_features] = scaler.fit_transform(X[numeric_features])\n",
    "\n",
    "# Define Train/Test split\n",
    "test_size=0.2\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "# Ensure the test set has the same data types as the training set\n",
    "X_test[numeric_features] = X_test[numeric_features].apply(pd.to_numeric, errors='coerce')\n",
    "X_test[numeric_features] = X_test[numeric_features].fillna(X_test[numeric_features].mean())\n",
    "for feature in categorical_features:\n",
    "    X_test[feature] = X_test[feature].astype('category')\n",
    "\n",
    "\n",
    "########################################################################################################################################################\n",
    "\n",
    "## - Apply one hot encoding to categorical features\n",
    "## - Separate dataframe into X_train_processed, X_test_processed, y_train, y_test\n",
    "##- Convert back to pandas dataframes X_train_processed_df, X_test_processed_df, y_train_df, y_test_df\n",
    "\n",
    "## This data is ready for fitting models\n",
    "\n",
    "# Create a ColumnTransformer to apply transformations to features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Get the column names after transformation\n",
    "transformed_columns = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Remove the prefixes from the column names\n",
    "transformed_columns = [col.split('__')[-1] for col in transformed_columns]\n",
    "\n",
    "# Convert the transformed arrays back to pandas DataFrames with column names - suitable for AIF360 usage\n",
    "X_train_processed_df = pd.DataFrame(X_train_processed, columns=transformed_columns)\n",
    "X_test_processed_df = pd.DataFrame(X_test_processed, columns=transformed_columns)\n",
    "\n",
    "# Ensure the target variable is also in DataFrame format\n",
    "y_train_df = pd.DataFrame(y_train).reset_index(drop=True)\n",
    "y_test_df = pd.DataFrame(y_test).reset_index(drop=True)\n",
    "\n",
    "\n",
    "####################################################################################################################################################\n",
    "\n",
    "## Create a StandardDataset compatible with the AIF360 package\n",
    "\n",
    "# Updated categorical_features with 'RACE_ETHNICITY' removed\n",
    "# Important to ensure that ['RACE_ETHNICITY'] is not listed as a categorical feature\n",
    "categorical_features_standard = [\n",
    "    'PROVIDER_RISK_HIERARCHY', 'AGE', 'LANG', 'EDUC', \n",
    "    'PLANTYPE', 'SMOKENOW', 'GENDER', 'SURVEY_MODE'\n",
    "]\n",
    "\n",
    "numeric_features = X.columns.drop(categorical_features_standard)\n",
    "columns_to_keep = df_cleaned_final.columns\n",
    "\n",
    "df_test = df_cleaned_final[columns_to_keep]\n",
    "\n",
    "# Map the racial values to integers as required by the fairness algorithm\n",
    "race_ethnicity_map = {'White': 0, 'Black': 1, 'Asian': 2, 'AIAN': 3, 'Latinx': 4, 'PacIsl': 5, 'TwoPlus': 6}\n",
    "\n",
    "# Converting 'PCM_DETRACTOR' and 'RACE_ETHNICITY' to numeric types\n",
    "df_test['PCM_DETRACTOR'] = df_test['PCM_DETRACTOR'].astype(float)\n",
    "df_test['RACE_ETHNICITY'] = df_test['RACE_ETHNICITY'].map(race_ethnicity_map)\n",
    "df_test['RACE_ETHNICITY'] = df_test['RACE_ETHNICITY'].astype(int)  \n",
    "\n",
    "df_test = pd.DataFrame(df_test)\n",
    "\n",
    "# Create a StandardDataset\n",
    "dataset = StandardDataset(df_test, \n",
    "                          label_name='PCM_DETRACTOR', \n",
    "                          favorable_classes=[0],  # 0 ('White') is the favorable class\n",
    "                          protected_attribute_names=['RACE_ETHNICITY'], \n",
    "                          privileged_classes=[[0]],  # 0 ('White') is the privileged class\n",
    "                          categorical_features=categorical_features_standard)\n",
    "\n",
    "\n",
    "\n",
    "# Specify the privileged and unprivileged group\n",
    "privileged_group = [{'RACE_ETHNICITY': 0}]  # White\n",
    "unprivileged_groups = [{'RACE_ETHNICITY': value} for key, value in race_ethnicity_map.items() if value != 0]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train, test = dataset.split([0.8], shuffle=True)\n",
    "\n",
    "# Extract features and labels from the StandardDataset\n",
    "X_train = train.features\n",
    "y_train = train.labels.ravel()\n",
    "\n",
    "X_test = test.features\n",
    "y_test = test.labels.ravel()\n",
    "\n",
    "##########################################################################################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e789acab-8c2d-48ef-b1b4-6d48ac267df8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Reject Option-Based Classification (ROC)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9870db6b-c295-4acf-881b-2a7fbdcb7663",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/xgboost/core.py:729: UserWarning: [14:56:30] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  return func(**kwargs)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6706880531564274"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the hyperparameters for XGBoost\n",
    "params = {\n",
    "    'learning_rate': 0.003, \n",
    "    'max_depth': 5,\n",
    "    'n_estimators': 5000,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    'scale_pos_weight': 9,  # Adjusted for class imbalance\n",
    "    'max_delta_step': 1,\n",
    "    'eval_metric': 'aucpr',    \n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda'     \n",
    "}\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_model_roc = XGBClassifier(**params)\n",
    "\n",
    "# Fit the model on the training data\n",
    "xgb_model_roc.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "y_pred = xgb_model_roc.predict(X_test)\n",
    "y_pred_proba = xgb_model_roc.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate AUC\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4144b2f-e392-47df-98af-79f62ad43a38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6920363785728942>, line 11\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Apply ROC post-processing\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m roc \u001B[38;5;241m=\u001B[39m RejectOptionClassification(\n",
       "\u001B[1;32m      3\u001B[0m     unprivileged_groups\u001B[38;5;241m=\u001B[39munprivileged_groups,\n",
       "\u001B[1;32m      4\u001B[0m     privileged_groups\u001B[38;5;241m=\u001B[39mprivileged_group,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m      8\u001B[0m     metric_ub\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.05\u001B[39m, metric_lb\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m0.05\u001B[39m\n",
       "\u001B[1;32m      9\u001B[0m )\n",
       "\u001B[0;32m---> 11\u001B[0m roc \u001B[38;5;241m=\u001B[39m roc\u001B[38;5;241m.\u001B[39mfit(test, y_pred_proba)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-67faf9ed-3fd0-4da9-91f1-e7a71d86343c/lib/python3.11/site-packages/aif360/algorithms/transformer.py:27\u001B[0m, in \u001B[0;36maddmetadata.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     25\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n",
       "\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n",
       "\u001B[0;32m---> 27\u001B[0m     new_dataset \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     28\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(new_dataset, Dataset):\n",
       "\u001B[1;32m     29\u001B[0m         new_dataset\u001B[38;5;241m.\u001B[39mmetadata \u001B[38;5;241m=\u001B[39m new_dataset\u001B[38;5;241m.\u001B[39mmetadata\u001B[38;5;241m.\u001B[39mcopy()\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-67faf9ed-3fd0-4da9-91f1-e7a71d86343c/lib/python3.11/site-packages/aif360/algorithms/postprocessing/reject_option_classification.py:127\u001B[0m, in \u001B[0;36mRejectOptionClassification.fit\u001B[0;34m(self, dataset_true, dataset_pred)\u001B[0m\n",
       "\u001B[1;32m    124\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mROC_margin \u001B[38;5;241m=\u001B[39m ROC_margin\n",
       "\u001B[1;32m    126\u001B[0m \u001B[38;5;66;03m# Predict using the current threshold and margin\u001B[39;00m\n",
       "\u001B[0;32m--> 127\u001B[0m dataset_transf_pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredict(dataset_pred)\n",
       "\u001B[1;32m    129\u001B[0m dataset_transf_metric_pred \u001B[38;5;241m=\u001B[39m BinaryLabelDatasetMetric(\n",
       "\u001B[1;32m    130\u001B[0m                              dataset_transf_pred,\n",
       "\u001B[1;32m    131\u001B[0m                              unprivileged_groups\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munprivileged_groups,\n",
       "\u001B[1;32m    132\u001B[0m                              privileged_groups\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprivileged_groups)\n",
       "\u001B[1;32m    133\u001B[0m classified_transf_metric \u001B[38;5;241m=\u001B[39m ClassificationMetric(\n",
       "\u001B[1;32m    134\u001B[0m                              dataset_true,\n",
       "\u001B[1;32m    135\u001B[0m                              dataset_transf_pred,\n",
       "\u001B[1;32m    136\u001B[0m                              unprivileged_groups\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munprivileged_groups,\n",
       "\u001B[1;32m    137\u001B[0m                              privileged_groups\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprivileged_groups)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-67faf9ed-3fd0-4da9-91f1-e7a71d86343c/lib/python3.11/site-packages/aif360/algorithms/transformer.py:27\u001B[0m, in \u001B[0;36maddmetadata.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     25\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n",
       "\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n",
       "\u001B[0;32m---> 27\u001B[0m     new_dataset \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     28\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(new_dataset, Dataset):\n",
       "\u001B[1;32m     29\u001B[0m         new_dataset\u001B[38;5;241m.\u001B[39mmetadata \u001B[38;5;241m=\u001B[39m new_dataset\u001B[38;5;241m.\u001B[39mmetadata\u001B[38;5;241m.\u001B[39mcopy()\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-67faf9ed-3fd0-4da9-91f1-e7a71d86343c/lib/python3.11/site-packages/aif360/algorithms/postprocessing/reject_option_classification.py:181\u001B[0m, in \u001B[0;36mRejectOptionClassification.predict\u001B[0;34m(self, dataset)\u001B[0m\n",
       "\u001B[1;32m    170\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, dataset):\n",
       "\u001B[1;32m    171\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Obtain fair predictions using the ROC method.\u001B[39;00m\n",
       "\u001B[1;32m    172\u001B[0m \n",
       "\u001B[1;32m    173\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    179\u001B[0m \u001B[38;5;124;03m        fair predictions obtain using the ROC method.\u001B[39;00m\n",
       "\u001B[1;32m    180\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 181\u001B[0m     dataset_new \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39mcopy(deepcopy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
       "\u001B[1;32m    183\u001B[0m     fav_pred_inds \u001B[38;5;241m=\u001B[39m (dataset\u001B[38;5;241m.\u001B[39mscores \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclassification_threshold)\n",
       "\u001B[1;32m    184\u001B[0m     unfav_pred_inds \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m~\u001B[39mfav_pred_inds\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: copy() got an unexpected keyword argument 'deepcopy'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "TypeError",
        "evalue": "copy() got an unexpected keyword argument 'deepcopy'"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: copy() got an unexpected keyword argument 'deepcopy'"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-6920363785728942>, line 11\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Apply ROC post-processing\u001B[39;00m\n\u001B[1;32m      2\u001B[0m roc \u001B[38;5;241m=\u001B[39m RejectOptionClassification(\n\u001B[1;32m      3\u001B[0m     unprivileged_groups\u001B[38;5;241m=\u001B[39munprivileged_groups,\n\u001B[1;32m      4\u001B[0m     privileged_groups\u001B[38;5;241m=\u001B[39mprivileged_group,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      8\u001B[0m     metric_ub\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.05\u001B[39m, metric_lb\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m0.05\u001B[39m\n\u001B[1;32m      9\u001B[0m )\n\u001B[0;32m---> 11\u001B[0m roc \u001B[38;5;241m=\u001B[39m roc\u001B[38;5;241m.\u001B[39mfit(test, y_pred_proba)\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-67faf9ed-3fd0-4da9-91f1-e7a71d86343c/lib/python3.11/site-packages/aif360/algorithms/transformer.py:27\u001B[0m, in \u001B[0;36maddmetadata.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 27\u001B[0m     new_dataset \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(new_dataset, Dataset):\n\u001B[1;32m     29\u001B[0m         new_dataset\u001B[38;5;241m.\u001B[39mmetadata \u001B[38;5;241m=\u001B[39m new_dataset\u001B[38;5;241m.\u001B[39mmetadata\u001B[38;5;241m.\u001B[39mcopy()\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-67faf9ed-3fd0-4da9-91f1-e7a71d86343c/lib/python3.11/site-packages/aif360/algorithms/postprocessing/reject_option_classification.py:127\u001B[0m, in \u001B[0;36mRejectOptionClassification.fit\u001B[0;34m(self, dataset_true, dataset_pred)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mROC_margin \u001B[38;5;241m=\u001B[39m ROC_margin\n\u001B[1;32m    126\u001B[0m \u001B[38;5;66;03m# Predict using the current threshold and margin\u001B[39;00m\n\u001B[0;32m--> 127\u001B[0m dataset_transf_pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredict(dataset_pred)\n\u001B[1;32m    129\u001B[0m dataset_transf_metric_pred \u001B[38;5;241m=\u001B[39m BinaryLabelDatasetMetric(\n\u001B[1;32m    130\u001B[0m                              dataset_transf_pred,\n\u001B[1;32m    131\u001B[0m                              unprivileged_groups\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munprivileged_groups,\n\u001B[1;32m    132\u001B[0m                              privileged_groups\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprivileged_groups)\n\u001B[1;32m    133\u001B[0m classified_transf_metric \u001B[38;5;241m=\u001B[39m ClassificationMetric(\n\u001B[1;32m    134\u001B[0m                              dataset_true,\n\u001B[1;32m    135\u001B[0m                              dataset_transf_pred,\n\u001B[1;32m    136\u001B[0m                              unprivileged_groups\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munprivileged_groups,\n\u001B[1;32m    137\u001B[0m                              privileged_groups\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprivileged_groups)\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-67faf9ed-3fd0-4da9-91f1-e7a71d86343c/lib/python3.11/site-packages/aif360/algorithms/transformer.py:27\u001B[0m, in \u001B[0;36maddmetadata.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 27\u001B[0m     new_dataset \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(new_dataset, Dataset):\n\u001B[1;32m     29\u001B[0m         new_dataset\u001B[38;5;241m.\u001B[39mmetadata \u001B[38;5;241m=\u001B[39m new_dataset\u001B[38;5;241m.\u001B[39mmetadata\u001B[38;5;241m.\u001B[39mcopy()\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-67faf9ed-3fd0-4da9-91f1-e7a71d86343c/lib/python3.11/site-packages/aif360/algorithms/postprocessing/reject_option_classification.py:181\u001B[0m, in \u001B[0;36mRejectOptionClassification.predict\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, dataset):\n\u001B[1;32m    171\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Obtain fair predictions using the ROC method.\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \n\u001B[1;32m    173\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    179\u001B[0m \u001B[38;5;124;03m        fair predictions obtain using the ROC method.\u001B[39;00m\n\u001B[1;32m    180\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 181\u001B[0m     dataset_new \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39mcopy(deepcopy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    183\u001B[0m     fav_pred_inds \u001B[38;5;241m=\u001B[39m (dataset\u001B[38;5;241m.\u001B[39mscores \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclassification_threshold)\n\u001B[1;32m    184\u001B[0m     unfav_pred_inds \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m~\u001B[39mfav_pred_inds\n",
        "\u001B[0;31mTypeError\u001B[0m: copy() got an unexpected keyword argument 'deepcopy'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply ROC post-processing\n",
    "roc = RejectOptionClassification(\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_group,\n",
    "    low_class_thresh=0.3, high_class_thresh=0.8,\n",
    "    num_class_thresh=100, num_ROC_margin=50,\n",
    "    metric_name=\"Statistical parity difference\",\n",
    "    metric_ub=0.05, metric_lb=-0.05\n",
    ")\n",
    "\n",
    "roc = roc.fit(test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d292e16b-e641-44be-940a-d41212297e10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate AUC\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "657e0af7-f70e-4277-926e-e56f7cabf1be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###############################################################  FIT MODEL   ###################################################################################\n",
    "\n",
    "# Define the hyperparameters for XGBoost\n",
    "params = {\n",
    "    'learning_rate': 0.003, \n",
    "    'max_depth': 5,\n",
    "    'n_estimators': 5000,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    'scale_pos_weight': 9,  # Adjusted for class imbalance\n",
    "    'max_delta_step': 1,\n",
    "    'eval_metric': 'aucpr',    \n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda'     \n",
    "}\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_model = XGBClassifier(**params)\n",
    "\n",
    "# Fit the model on the training data\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "###############################################################  PERFORMANCE METRICS  ###################################################################################\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]  # Get the probability predictions for AUC\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Assuming y_pred_proba and y_test are already defined\n",
    "# Define a range of thresholds\n",
    "thresholds = [i / 100 for i in range(10, 90)]  # Thresholds \n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Loop through each threshold and calculate metrics\n",
    "for threshold in thresholds:\n",
    "    # Apply the threshold to get the final class labels\n",
    "    y_pred_custom_threshold = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate evaluation metrics with the custom threshold\n",
    "    accuracy_custom = accuracy_score(y_test, y_pred_custom_threshold)\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred_custom_threshold)\n",
    "    f1_custom = f1_score(y_test, y_pred_custom_threshold)\n",
    "    precision_custom = precision_score(y_test, y_pred_custom_threshold)\n",
    "    recall_custom = recall_score(y_test, y_pred_custom_threshold)\n",
    "    \n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'threshold': threshold,\n",
    "        'accuracy': accuracy_custom,\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "        'f1': f1_custom,\n",
    "        'recall': recall_custom,\n",
    "        'precision': precision_custom\n",
    "    })\n",
    "\n",
    "# Convert the results list to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Identify the threshold that gives the maximum F1 score\n",
    "max_f1_threshold = results_df.loc[results_df['f1'].idxmax()]['threshold']\n",
    "max_f1_value = results_df.loc[results_df['f1'].idxmax()]['f1']\n",
    "recall_value = results_df.loc[results_df['f1'].idxmax()]['recall']\n",
    "precision_value = results_df.loc[results_df['f1'].idxmax()]['precision']\n",
    "accuracy_value = results_df.loc[results_df['f1'].idxmax()]['accuracy']\n",
    "balanced_accuracy_value = results_df.loc[results_df['f1'].idxmax()]['balanced_accuracy']\n",
    "\n",
    "print(f\"The threshold that gives the maximum F1 score is: {max_f1_threshold}\")\n",
    "print(f\"The maximum F1 score is: {max_f1_value}\")\n",
    "print(f\"The auc is: {auc}\")\n",
    "print(f\"The balanced accuracy is: {balanced_accuracy_value}\")\n",
    "print(f\"The recall is: {recall_value}\")\n",
    "print(f\"The precision is: {precision_value}\")\n",
    "print(f\"The accuracy is: {accuracy_value}\")\n",
    "\n",
    "# Access the underlying XGBClassifier model\n",
    "underlying_model = xgb_model\n",
    "\n",
    "###############################################################  FAIRNESS METRICS  ###################################################################################\n",
    "\n",
    "# Convert predictions to a StandardDataset\n",
    "pred_dataset = test.copy()\n",
    "pred_dataset.labels = y_pred.reshape(-1, 1)\n",
    "\n",
    "# Initialize a list to store the results\n",
    "fairness_metrics = []\n",
    "\n",
    "for group in unprivileged_groups:\n",
    "    metric = ClassificationMetric(test, pred_dataset, unprivileged_groups=[group], privileged_groups=privileged_group)\n",
    "    \n",
    "    disparate_impact = metric.disparate_impact()\n",
    "    statistical_parity_difference = metric.statistical_parity_difference()\n",
    "    equalized_odds_difference = metric.equalized_odds_difference()\n",
    "    predictive_equality_difference = metric.false_positive_rate_difference()\n",
    "\n",
    "    fairness_metrics.append({\n",
    "        'group': group['RACE_ETHNICITY'],\n",
    "        'disparate_impact': disparate_impact,\n",
    "        'statistical_parity_difference': statistical_parity_difference,\n",
    "        'equalized_odds_difference': equalized_odds_difference,\n",
    "        'predictive_equality_difference': predictive_equality_difference\n",
    "        })\n",
    "    \n",
    "# Extract the values for each metric\n",
    "disparate_impact_values = [metric['disparate_impact'] for metric in fairness_metrics]\n",
    "statistical_parity_difference_values = [metric['statistical_parity_difference'] for metric in fairness_metrics]\n",
    "equalized_odds_difference_values = [metric['equalized_odds_difference'] for metric in fairness_metrics]\n",
    "predictive_equality_difference_values = [metric['predictive_equality_difference'] for metric in fairness_metrics]\n",
    "\n",
    "# Calculate the average and standard deviation for each metric\n",
    "average_disparate_impact = np.mean(disparate_impact_values)\n",
    "std_disparate_impact = np.std(disparate_impact_values)\n",
    "\n",
    "average_statistical_parity_difference = np.mean(statistical_parity_difference_values)\n",
    "std_statistical_parity_difference = np.std(statistical_parity_difference_values)\n",
    "\n",
    "average_equalized_odds_difference = np.mean(equalized_odds_difference_values)\n",
    "std_equalized_odds_difference = np.std(equalized_odds_difference_values)\n",
    "\n",
    "average_predictive_equality_difference = np.mean(predictive_equality_difference_values)\n",
    "std_predictive_equality_difference = np.std(predictive_equality_difference_values)\n",
    "\n",
    "print(f\"Average Disparate Impact: {average_disparate_impact} with a standard deviation of {std_disparate_impact}\")\n",
    "print(f\"Average Statistical Parity Difference: {average_statistical_parity_difference} with a standard deviation of {std_statistical_parity_difference}\")\n",
    "print(f\"Average Equalized Odds Difference: {average_equalized_odds_difference} with a standard deviation of {std_equalized_odds_difference}\")\n",
    "print(f\"Average Predictive Equality Difference: {average_predictive_equality_difference} with a standard deviation of {std_predictive_equality_difference}\")\n",
    "\n",
    "fairness_metrics"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Post_ROC_v1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}